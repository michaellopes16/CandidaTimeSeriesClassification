{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CTt1zjM_q92d",
    "outputId": "0d21448b-c2bf-45c6-9a20-9363e2ad36af"
   },
   "outputs": [],
   "source": [
    "from builtins import print\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import random\n",
    "\n",
    "\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "matplotlib.rcParams['font.sans-serif'] = 'Arial'\n",
    "\n",
    "import os\n",
    "import operator\n",
    "from keras.optimizers import adam_v2\n",
    "import inspect\n",
    "from tensorflow.python.keras.optimizers import adamax_v2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#import utils\n",
    "\n",
    "#from utils.constants import UNIVARIATE_DATASET_NAMES as DATASET_NAMES\n",
    "#from utils.constants import UNIVARIATE_ARCHIVE_NAMES as ARCHIVE_NAMES\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import keras\n",
    "from keras.layers.normalization.layer_normalization import *\n",
    "from keras.layers.normalization.batch_normalization import *\n",
    "#from utils.utils import calculate_metrics\n",
    "#from utils.utils import create_directory\n",
    "#from utils.utils import check_if_file_exits\n",
    "import gc\n",
    "import time\n",
    "\n",
    "#from utils.utils import save_logs\n",
    "#from utils.utils import save_test_duration\n",
    "\n",
    "#from utils.utils import read_all_datasets\n",
    "#from utils.utils import transform_labels\n",
    "#from utils.utils import create_directory\n",
    "#from utils.utils import run_length_xps\n",
    "#from utils.utils import generate_results_csv\n",
    ",\n",
    "import sys\n",
    "import sklearn\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lhsrUO457K2C",
    "outputId": "8fc794d1-affc-47f7-8e6a-a2bd38b8d1cf"
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.chdir(\"/content/drive/MyDrive/Colab Notebooks/Candidas/archives/TSC/AllCandidas/\")\n",
    "#!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDDr_FbD_oIz",
    "outputId": "5e910dde-8e4b-4eaa-e0d4-d9f44c356efc"
   },
   "outputs": [],
   "source": [
    "#open(\"/content/drive/MyDrive/Colab Notebooks/Candidas/archives/TSC/AllCandidas/AllCandidas_TRAIN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4z3uPGHKrXMC"
   },
   "outputs": [],
   "source": [
    "def check_if_file_exits(file_name):\n",
    "    return os.path.exists(file_name)\n",
    "\n",
    "\n",
    "def readucr(filename, delimiter=','):\n",
    "    print(filename)\n",
    "    data = np.loadtxt(filename, delimiter=delimiter)\n",
    "    Y = data[:, 0]\n",
    "    X = data[:, 1:]\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def readsits(filename, delimiter=','):\n",
    "    data = np.loadtxt(filename, delimiter=delimiter)\n",
    "    Y = data[:, -1]\n",
    "    X = data[:, :-1]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "esRD1dQTrf1e"
   },
   "outputs": [],
   "source": [
    "def create_directory(directory_path):\n",
    "    if os.path.exists(directory_path):\n",
    "        return None\n",
    "    else:\n",
    "        try:\n",
    "            os.makedirs(directory_path)\n",
    "        except:\n",
    "            # in case another machine created the path meanwhile !:(\n",
    "            return None\n",
    "        return directory_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yxzHJyO8rtmT"
   },
   "outputs": [],
   "source": [
    "def read_dataset(root_dir, archive_name, dataset_name):\n",
    "    datasets_dict = {}\n",
    "\n",
    "    file_name = root_dir + '/archives/' + archive_name + '/' + dataset_name + '/' + dataset_name\n",
    "    x_train, y_train = readucr(file_name + '_TRAIN.csv')\n",
    "    x_test, y_test = readucr(file_name + '_TEST.csv')\n",
    "    datasets_dict[dataset_name] = (x_train.copy(), y_train.copy(), x_test.copy(),\n",
    "                                   y_test.copy())\n",
    "\n",
    "    return datasets_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JB_-BQ2Sr1cg"
   },
   "outputs": [],
   "source": [
    "def read_all_datasets(root_dir, archive_name):\n",
    "    datasets_dict = {}\n",
    "\n",
    "    dataset_names_to_sort = []\n",
    "\n",
    "    if archive_name == 'TSC':\n",
    "        for dataset_name in UNIVARIATE_DATASET_NAMES:\n",
    "            root_dir_dataset = root_dir + '\\\\archives\\\\' + archive_name + '\\\\' + dataset_name + '\\\\'\n",
    "            file_name = root_dir_dataset + dataset_name\n",
    "            print(file_name)\n",
    "            x_train, y_train = readucr(file_name + '_TRAIN.csv')\n",
    "            x_test, y_test = readucr(file_name + '_TEST.csv')\n",
    "\n",
    "            datasets_dict[dataset_name] = (x_train.copy(), y_train.copy(), x_test.copy(),\n",
    "                                           y_test.copy())\n",
    "\n",
    "            dataset_names_to_sort.append((dataset_name, len(x_train)))\n",
    "\n",
    "        dataset_names_to_sort.sort(key=operator.itemgetter(1))\n",
    "\n",
    "        for i in range(len(UNIVARIATE_DATASET_NAMES)):\n",
    "            UNIVARIATE_DATASET_NAMES[i] = dataset_names_to_sort[i][0]\n",
    "\n",
    "#    elif archive_name == 'InlineSkateXPs':\n",
    "#\n",
    "#        for dataset_name in dataset_names_for_archive[archive_name]:\n",
    "#            root_dir_dataset = root_dir + '/archives/' + archive_name + '/' + dataset_name + '/'\n",
    "#\n",
    "#            x_train = np.load(root_dir_dataset + 'x_train.npy')\n",
    "#            y_train = np.load(root_dir_dataset + 'y_train.npy')\n",
    "#            x_test = np.load(root_dir_dataset + 'x_test.npy')\n",
    "#            y_test = np.load(root_dir_dataset + 'y_test.npy')\n",
    "#\n",
    "#            datasets_dict[dataset_name] = (x_train.copy(), y_train.copy(), x_test.copy(),\n",
    "#                                           y_test.copy())\n",
    "#    elif archive_name == 'SITS':\n",
    "#        return read_sits_xps(root_dir)\n",
    "#    else:\n",
    "#        print('error in archive name')\n",
    "#        exit()\n",
    "\n",
    "    return datasets_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "V9F82LTZsBen"
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, duration):\n",
    "    res = pd.DataFrame(data=np.zeros((1, 5), dtype=np.float), index=[0],\n",
    "                       columns=['precision', 'accuracy', 'recall','f1_score','duration'])\n",
    "    res['precision'] = precision_score(y_true, y_pred, average='macro')\n",
    "    res['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    res['recall'] = recall_score(y_true, y_pred, average='macro')\n",
    "    res['f1_score'] = f1_score(y_true, y_pred, average='macro')\n",
    "    res['duration'] = duration\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7T-lY3wZsBt5"
   },
   "outputs": [],
   "source": [
    "def save_test_duration(file_name, test_duration):\n",
    "    res = pd.DataFrame(data=np.zeros((1, 1), dtype=np.float), index=[0],\n",
    "                       columns=['test_duration'])\n",
    "    res['test_duration'] = test_duration\n",
    "    res.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gnUIJQ5ksB75"
   },
   "outputs": [],
   "source": [
    "def transform_labels(y_train, y_test):\n",
    "    \"\"\"\n",
    "    Transform label to min equal zero and continuous\n",
    "    For example if we have [1,3,4] --->  [0,1,2]\n",
    "    \"\"\"\n",
    "    # no validation split\n",
    "    # init the encoder\n",
    "    encoder = LabelEncoder()\n",
    "    # concat train and test to fit\n",
    "    y_train_test = np.concatenate((y_train, y_test), axis=0)\n",
    "    # fit the encoder\n",
    "    encoder.fit(y_train_test)\n",
    "    # transform to min zero and continuous labels\n",
    "    new_y_train_test = encoder.transform(y_train_test)\n",
    "    # resplit the train and test\n",
    "    new_y_train = new_y_train_test[0:len(y_train)]\n",
    "    new_y_test = new_y_train_test[len(y_train):]\n",
    "    return new_y_train, new_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ApmmF3b0scMR"
   },
   "outputs": [],
   "source": [
    "def generate_results_csv(output_file_name, root_dir, clfs):\n",
    "    res = pd.DataFrame(data=np.zeros((0, 8), dtype=np.float), index=[],\n",
    "                       columns=['classifier_name', 'archive_name', 'dataset_name', 'iteration',\n",
    "                                'precision', 'accuracy', 'recall', 'duration'])\n",
    "    for archive_name in UNIVARIATE_ARCHIVE_NAMES:\n",
    "        datasets_dict = read_all_datasets(root_dir, archive_name)\n",
    "        for classifier_name in clfs:\n",
    "            durr = 0.0\n",
    "\n",
    "            curr_archive_name = archive_name\n",
    "            for dataset_name in datasets_dict.keys():\n",
    "                output_dir = root_dir + 'results\\\\' + classifier_name + '\\\\' \\\n",
    "                             + curr_archive_name + '\\\\' + dataset_name + '\\\\' + 'df_metrics.csv'\n",
    "                print(output_dir)\n",
    "                if not os.path.exists(output_dir):\n",
    "                    continue\n",
    "                df_metrics = pd.read_csv(output_dir)\n",
    "                df_metrics['classifier_name'] = classifier_name\n",
    "                df_metrics['archive_name'] = archive_name\n",
    "                df_metrics['dataset_name'] = dataset_name\n",
    "                df_metrics['iteration'] = 0\n",
    "                res = pd.concat((res, df_metrics), axis=0, sort=False)\n",
    "                durr += df_metrics['duration'][0]\n",
    "\n",
    "    res.to_csv(root_dir + output_file_name, index=False)\n",
    "\n",
    "    res = res.loc[res['classifier_name'].isin(clfs)]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lYAf8aF9scW0"
   },
   "outputs": [],
   "source": [
    "def plot_epochs_metric(hist, file_name, metric='loss'):\n",
    "    plt.figure()\n",
    "    plt.plot(hist.history[metric])\n",
    "    plt.plot(hist.history['val_' + metric])\n",
    "    plt.title('model ' + metric)\n",
    "    plt.ylabel(metric, fontsize='large')\n",
    "    plt.xlabel('epoch', fontsize='large')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.savefig(file_name, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8CyqisiEscgC"
   },
   "outputs": [],
   "source": [
    "def save_logs(output_directory, hist, y_pred, y_true, duration,\n",
    "              lr=True, plot_test_acc=True):\n",
    "    hist_df = pd.DataFrame(hist.history)\n",
    "    hist_df.to_csv(output_directory + 'history.csv', index=False)\n",
    "\n",
    "    df_metrics = calculate_metrics(y_true, y_pred, duration)\n",
    "    df_metrics.to_csv(output_directory + 'df_metrics.csv', index=False)\n",
    "\n",
    "    index_best_model = hist_df['loss'].idxmin()\n",
    "    row_best_model = hist_df.loc[index_best_model]\n",
    "\n",
    "    df_best_model = pd.DataFrame(data=np.zeros((1, 6), dtype=np.float), index=[0],\n",
    "                                 columns=['best_model_train_loss', 'best_model_val_loss', 'best_model_train_acc',\n",
    "                                          'best_model_val_acc', 'best_model_learning_rate', 'best_model_nb_epoch'])\n",
    "\n",
    "    df_best_model['best_model_train_loss'] = row_best_model['loss']\n",
    "    #if plot_test_acc:\n",
    "    #    df_best_model['best_model_val_loss'] = row_best_model['val_loss']\n",
    "    #df_best_model['best_model_train_acc'] = row_best_model['acc']\n",
    "    if plot_test_acc:\n",
    "        df_best_model['best_model_val_acc'] = row_best_model['val_acc']\n",
    "    if lr == True:\n",
    "        df_best_model['best_model_learning_rate'] = row_best_model['lr']\n",
    "    df_best_model['best_model_nb_epoch'] = index_best_model\n",
    "\n",
    "    df_best_model.to_csv(output_directory + 'df_best_model.csv', index=False)\n",
    "\n",
    "    if plot_test_acc:\n",
    "        # plot losses\n",
    "        plot_epochs_metric(hist, output_directory + 'epochs_loss.png')\n",
    "\n",
    "    return df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "J9uNdVgzs1mo"
   },
   "outputs": [],
   "source": [
    "def create_synthetic_dataset(pattern_len=[0.25], pattern_pos=[0.1, 0.65], ts_len=128, ts_n=128):\n",
    "    random.seed(1234)\n",
    "    np.random.seed(1234)\n",
    "\n",
    "    nb_classes = len(pattern_pos) * len(pattern_len)\n",
    "\n",
    "    out_dir = '/b/home/uha/hfawaz-datas/dl-tsc/archives/UCRArchive_2018/BinaryData/'\n",
    "\n",
    "    create_directory(out_dir)\n",
    "\n",
    "    x_train = np.random.normal(0.0, 0.1, size=(ts_n, ts_len))\n",
    "    x_test = np.random.normal(0.0, 0.1, size=(ts_n, ts_len))\n",
    "\n",
    "    y_train = np.random.randint(low=0, high=nb_classes, size=(ts_n,))\n",
    "    y_test = np.random.randint(low=0, high=nb_classes, size=(ts_n,))\n",
    "\n",
    "    # make sure at least each class has one example\n",
    "    y_train[:nb_classes] = np.arange(start=0, stop=nb_classes, dtype=np.int32)\n",
    "    y_test[:nb_classes] = np.arange(start=0, stop=nb_classes, dtype=np.int32)\n",
    "\n",
    "    # each class is defined with a certain combination of pattern_pos and pattern_len\n",
    "    # with one pattern_len and two pattern_pos we can create only two classes\n",
    "    # example:  class 0 _____-_  & class 1 _-_____\n",
    "\n",
    "    # create the class definitions\n",
    "    class_def = [None for i in range(nb_classes)]\n",
    "\n",
    "    idx_class = 0\n",
    "    for pl in pattern_len:\n",
    "        for pp in pattern_pos:\n",
    "            class_def[idx_class] = {'pattern_len': int(pl * ts_len),\n",
    "                                    'pattern_pos': int(pp * ts_len)}\n",
    "            idx_class += 1\n",
    "\n",
    "    # create the dataset\n",
    "    for i in range(ts_n):\n",
    "        # for the train\n",
    "        c = y_train[i]\n",
    "        curr_pattern_pos = class_def[c]['pattern_pos']\n",
    "        curr_pattern_len = class_def[c]['pattern_len']\n",
    "        x_train[i][curr_pattern_pos:curr_pattern_pos + curr_pattern_len] = \\\n",
    "            x_train[i][curr_pattern_pos:curr_pattern_pos + curr_pattern_len] + 1.0\n",
    "\n",
    "        # for the test\n",
    "        c = y_test[i]\n",
    "        curr_pattern_pos = class_def[c]['pattern_pos']\n",
    "        curr_pattern_len = class_def[c]['pattern_len']\n",
    "        x_test[i][curr_pattern_pos:curr_pattern_pos + curr_pattern_len] = \\\n",
    "            x_test[i][curr_pattern_pos:curr_pattern_pos + curr_pattern_len] + 1.0\n",
    "\n",
    "    # znorm\n",
    "    x_train = (x_train - x_train.mean(axis=1, keepdims=True)) \\\n",
    "              / x_train.std(axis=1, keepdims=True)\n",
    "\n",
    "    x_test = (x_test - x_test.mean(axis=1, keepdims=True)) \\\n",
    "             / x_test.std(axis=1, keepdims=True)\n",
    "\n",
    "    # visualize example\n",
    "    # plt.figure()\n",
    "    # colors = generate_array_of_colors(nb_classes)\n",
    "    # for c in range(nb_classes):\n",
    "    #     plt.plot(x_train[y_train == c][0], color=colors[c], label='class-' + str(c))\n",
    "    # plt.legend(loc='best')\n",
    "    # plt.savefig('out.pdf')\n",
    "    # exit()\n",
    "\n",
    "    # np.save(out_dir+'x_train.npy',x_train)\n",
    "    # np.save(out_dir+'y_train.npy',y_train)\n",
    "    # np.save(out_dir+'x_test.npy',x_test)\n",
    "    # np.save(out_dir+'y_test.npy',y_test)\n",
    "\n",
    "    # print('Done creating dataset!')\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "sPgcX9m4s1sK"
   },
   "outputs": [],
   "source": [
    "def generate_array_of_colors(n):\n",
    "    # https://www.quora.com/How-do-I-generate-n-visually-distinct-RGB-colours-in-Python\n",
    "    ret = []\n",
    "    r = int(random.random() * 256)\n",
    "    g = int(random.random() * 256)\n",
    "    b = int(random.random() * 256)\n",
    "    alpha = 1.0\n",
    "    step = 256 / n\n",
    "    for i in range(n):\n",
    "        r += step\n",
    "        g += step\n",
    "        b += step\n",
    "        r = int(r) % 256\n",
    "        g = int(g) % 256\n",
    "        b = int(b) % 256\n",
    "        ret.append((r / 255, g / 255, b / 255, alpha))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7WRYP7qfs1x4"
   },
   "outputs": [],
   "source": [
    "def read_sits_xps(root_dir):\n",
    "    datasets_dict = {}\n",
    "    path_to_data = root_dir + 'archives\\\\SITS\\\\resampled-SITS\\\\'\n",
    "    path_to_test = root_dir + 'archives\\\\SITS\\\\' + 'SatelliteFull_TEST_1000.csv'\n",
    "\n",
    "    x_test, y_test = readsits(path_to_test)\n",
    "\n",
    "    for subdir, dirs, files in os.walk(path_to_data):\n",
    "        for file_name in files:\n",
    "            arr = file_name.split('.')\n",
    "            dataset_name = arr[0]\n",
    "            file_type = arr[1]\n",
    "            if file_type == 'csv':\n",
    "                x_train, y_train = readsits(subdir + '/' + file_name)\n",
    "\n",
    "                datasets_dict[dataset_name] = (x_train.copy(), y_train.copy(), x_test.copy(),\n",
    "                                               y_test.copy())\n",
    "\n",
    "    return datasets_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ckLDgRLzs17H"
   },
   "outputs": [],
   "source": [
    "def resample_dataset(x, rate):\n",
    "    new_x = np.zeros(shape=(x.shape[0], rate))\n",
    "    from scipy import signal\n",
    "    for i in range(x.shape[0]):\n",
    "        f = signal.resample(x[0], rate)\n",
    "        new_x[i] = f\n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "GApt7T0MtRF2"
   },
   "outputs": [],
   "source": [
    "def run_length_xps(root_dir):\n",
    "    #archive_name = ARCHIVE_NAMES[0]\n",
    "    archive_name = UNIVARIATE_ARCHIVE_NAMES[0]\n",
    "    dataset_name = 'InlineSkate'\n",
    "    datasets_dict = read_dataset(root_dir, archive_name, dataset_name)\n",
    "\n",
    "    lengths = [2 ** i for i in range(5, 12)]\n",
    "\n",
    "    x_train = datasets_dict[dataset_name][0]\n",
    "    y_train = datasets_dict[dataset_name][1]\n",
    "    x_test = datasets_dict[dataset_name][2]\n",
    "    y_test = datasets_dict[dataset_name][3]\n",
    "\n",
    "    new_archive_name = 'TSC'\n",
    "#    new_archive_name = 'InlineSkateXPs'\n",
    "\n",
    "    for l in lengths:\n",
    "        new_x_train = resample_dataset(x_train, l)\n",
    "        new_x_test = resample_dataset(x_test, l)\n",
    "        new_dataset_name = dataset_name + '-' + str(l)\n",
    "        new_dataset_dir = root_dir + 'archives\\\\' + new_archive_name + '\\\\' + new_dataset_name + '\\\\'\n",
    "        create_directory(new_dataset_dir)\n",
    "\n",
    "        np.save(new_dataset_dir + 'x_train.npy', new_x_train)\n",
    "        np.save(new_dataset_dir + 'y_train.npy', y_train)\n",
    "        np.save(new_dataset_dir + 'x_test.npy', new_x_test)\n",
    "        np.save(new_dataset_dir + 'y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "A1yznVGrtRT4"
   },
   "outputs": [],
   "source": [
    "#UNIVARIATE_DATASET_NAMES = ['50words', 'Adiac', 'ArrowHead', 'Beef', 'BeetleFly', 'BirdChicken', 'Car', 'CBF',\n",
    "#                            'ChlorineConcentration', 'CinC_ECG_torso', 'Coffee',\n",
    "#                            'Computers', 'Cricket_X', 'Cricket_Y', 'Cricket_Z', 'DiatomSizeReduction',\n",
    "#                            'DistalPhalanxOutlineAgeGroup', 'DistalPhalanxOutlineCorrect', 'DistalPhalanxTW',\n",
    "#                            'Earthquakes', 'ECG200', 'ECG5000', 'ECGFiveDays', 'ElectricDevices', 'FaceAll', 'FaceFour',\n",
    "#                            'FacesUCR', 'FISH', 'FordA', 'FordB', 'Gun_Point', 'Ham', 'HandOutlines',\n",
    "#                            'Haptics', 'Herring', 'InlineSkate', 'InsectWingbeatSound', 'ItalyPowerDemand',\n",
    "#                            'LargeKitchenAppliances', 'Lighting2', 'Lighting7', 'MALLAT', 'Meat', 'MedicalImages',\n",
    "#                            'MiddlePhalanxOutlineAgeGroup', 'MiddlePhalanxOutlineCorrect', 'MiddlePhalanxTW',\n",
    "#                            'MoteStrain', 'NonInvasiveFatalECG_Thorax1', 'NonInvasiveFatalECG_Thorax2', 'OliveOil',\n",
    "#                            'OSULeaf', 'PhalangesOutlinesCorrect', 'Phoneme', 'Plane', 'ProximalPhalanxOutlineAgeGroup',\n",
    "#                            'ProximalPhalanxOutlineCorrect', 'ProximalPhalanxTW', 'RefrigerationDevices',\n",
    "#                            'ScreenType', 'ShapeletSim', 'ShapesAll', 'SmallKitchenAppliances', 'SonyAIBORobotSurface',\n",
    "#                            'SonyAIBORobotSurfaceII', 'StarLightCurves', 'Strawberry', 'SwedishLeaf', 'Symbols',\n",
    "#                            'synthetic_control', 'ToeSegmentation1', 'ToeSegmentation2', 'Trace', 'TwoLeadECG',\n",
    "#                            'Two_Patterns', 'UWaveGestureLibraryAll', 'uWaveGestureLibrary_X', 'uWaveGestureLibrary_Y',\n",
    "#                            'uWaveGestureLibrary_Z', 'wafer', 'Wine', 'WordsSynonyms', 'Worms', 'WormsTwoClass', 'yoga']\n",
    "\n",
    "#UNIVARIATE_DATASET_NAMES = ['ElectricDevices', 'FordA', 'NonInvasiveFatalECG_Thorax1', 'NonInvasiveFatalECG_Thorax2',\n",
    "#                            'ProximalPhalanxOutlineCorrect', 'Two_Patterns', 'wafer']\n",
    "\n",
    "UNIVARIATE_DATASET_NAMES = ['AllCandidas']\n",
    "\n",
    "#UNIVARIATE_DATASET_NAMES = ['bases_10062021_anem_sens']\n",
    "\n",
    "#UNIVARIATE_DATASET_NAMES = ['bases_10062021_det', 'bases_10062021']\n",
    "\n",
    "#UNIVARIATE_DATASET_NAMES = ['nariz_nov2020']\n",
    "\n",
    "#UNIVARIATE_DATASET_NAMES = ['renan_junto']\n",
    "\n",
    "#UNIVARIATE_DATASET_NAMES = ['nariz_nov2020', 'renan_junto', 'renan_nao_padrao', 'renan_padrao']\n",
    "\n",
    "#UNIVARIATE_DATASET_NAMES = ['nariz_1120_9cl_10', 'nariz_1120_9cl_20', 'nariz_1120_9cl_30', 'nariz_1120_9cl_40',\n",
    "#                            'nariz_1120_9cl_60', 'nariz_1120_9cl_70', 'nariz_1120_9cl_80', 'nariz_1120_9cl_90']\n",
    "\n",
    "#UNIVARIATE_DATASET_NAMES = ['nariz_nov2020_sem_cl5']\n",
    "\n",
    "#UNIVARIATE_DATASET_NAMES = ['PhalangesOutlinesCorrect']\n",
    "#UNIVARIATE_DATASET_NAMES = ['Coffee']\n",
    "\n",
    "#UNIVARIATE_DATASET_NAMES = ['Meat', 'Coffee']\n",
    "\n",
    "UNIVARIATE_ARCHIVE_NAMES = ['TSC', 'InlineSkateXPs', 'SITS']\n",
    "#UNIVARIATE_ARCHIVE_NAMES = ['TSC']\n",
    "\n",
    "SITS_DATASETS = ['SatelliteFull_TRAIN_c301', 'SatelliteFull_TRAIN_c200', 'SatelliteFull_TRAIN_c451',\n",
    "                 'SatelliteFull_TRAIN_c89', 'SatelliteFull_TRAIN_c677', 'SatelliteFull_TRAIN_c59',\n",
    "                 'SatelliteFull_TRAIN_c133']\n",
    "\n",
    "InlineSkateXPs_DATASETS = ['InlineSkate-32', 'InlineSkate-64', 'InlineSkate-128',\n",
    "                           'InlineSkate-256', 'InlineSkate-512', 'InlineSkate-1024',\n",
    "                           'InlineSkate-2048']\n",
    "\n",
    "dataset_names_for_archive = {'TSC': UNIVARIATE_DATASET_NAMES,\n",
    "                             'SITS': SITS_DATASETS,\n",
    "                             'InlineSkateXPs': InlineSkateXPs_DATASETS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "PLqd9yE9q0cw"
   },
   "outputs": [],
   "source": [
    "class Classifier_NNE:\n",
    "\n",
    "    def create_classifier(self, model_name, input_shape, nb_classes, output_directory, verbose=False,\n",
    "                          build=True):\n",
    "        if self.check_if_match('inception*', model_name):\n",
    "            #from classifiers import inception\n",
    "            return Classifier_INCEPTION(output_directory, input_shape, nb_classes, verbose,build=build)\n",
    "\n",
    "    def check_if_match(self, rex, name2):\n",
    "        import re\n",
    "        pattern = re.compile(rex)\n",
    "        return pattern.match(name2)\n",
    "\n",
    "    def __init__(self, output_directory, input_shape, nb_classes, verbose=False, nb_iterations=5,\n",
    "                 clf_name='inception'):\n",
    "        self.classifiers = [clf_name]\n",
    "        out_add = ''\n",
    "        for cc in self.classifiers:\n",
    "            out_add = out_add + cc + '-'\n",
    "        self.archive_name = UNIVARIATE_ARCHIVE_NAMES[0]\n",
    "        self.iterations_to_take = [i for i in range(nb_iterations)]\n",
    "        for cc in self.iterations_to_take:\n",
    "            out_add = out_add + str(cc) + '-'\n",
    "        self.output_directory = output_directory.replace('nne',\n",
    "                                                         'nne' + '\\\\' + out_add)\n",
    "        create_directory(self.output_directory)\n",
    "        self.dataset_name = output_directory.split('\\\\')[-2]\n",
    "        self.verbose = verbose\n",
    "        self.models_dir = output_directory.replace('nne', 'classifier')\n",
    "\n",
    "    def fit(self, x_train, y_train, x_test, y_test, y_true):\n",
    "        # no training since models are pre-trained\n",
    "        start_time = time.time()\n",
    "\n",
    "        y_pred = np.zeros(shape=y_test.shape)\n",
    "\n",
    "        ll = 0\n",
    "\n",
    "        # loop through all classifiers\n",
    "        for model_name in self.classifiers:\n",
    "            # loop through different initialization of classifiers\n",
    "            for itr in self.iterations_to_take:\n",
    "                if itr == 0:\n",
    "                    itr_str = ''\n",
    "                else:\n",
    "                    itr_str = '_itr_' + str(itr)\n",
    "\n",
    "                curr_archive_name = self.archive_name + itr_str\n",
    "\n",
    "                curr_dir = self.models_dir.replace('classifier', model_name).replace(\n",
    "                    self.archive_name, curr_archive_name)\n",
    "\n",
    "                model = self.create_classifier(model_name, None, None,\n",
    "                                               curr_dir, build=False)\n",
    "\n",
    "                predictions_file_name = curr_dir + 'y_pred.npy'\n",
    "                # check if predictions already made\n",
    "                if check_if_file_exits(predictions_file_name):\n",
    "                    # then load only the predictions from the file\n",
    "                    curr_y_pred = np.load(predictions_file_name)\n",
    "                else:\n",
    "                    # then compute the predictions\n",
    "                    curr_y_pred = model.predict(x_test, y_true, x_train, y_train, y_test,\n",
    "                                                return_df_metrics=False)\n",
    "                    keras.backend.clear_session()\n",
    "\n",
    "                    np.save(predictions_file_name, curr_y_pred)\n",
    "\n",
    "                y_pred = y_pred + curr_y_pred\n",
    "\n",
    "                ll += 1\n",
    "\n",
    "        # average predictions\n",
    "        y_pred = y_pred / ll\n",
    "\n",
    "        # save predictions\n",
    "        np.save(self.output_directory + 'y_pred.npy', y_pred)\n",
    "\n",
    "        # convert the predicted from binary to integer\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        df_metrics = calculate_metrics(y_true, y_pred, duration)\n",
    "\n",
    "        df_metrics.to_csv(self.output_directory + 'df_metrics.csv', index=False)\n",
    "\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "UdNdNnM4tSUq"
   },
   "outputs": [],
   "source": [
    "class Classifier_INCEPTION:\n",
    "\n",
    "    def __init__(self, output_directory, input_shape, nb_classes, verbose=False, build=True, batch_size=64,\n",
    "                 nb_filters=32, use_residual=True, use_bottleneck=True, depth=6, kernel_size=41, nb_epochs=1500):\n",
    "\n",
    "        self.output_directory = output_directory\n",
    "\n",
    "        self.nb_filters = nb_filters\n",
    "        self.use_residual = use_residual\n",
    "        self.use_bottleneck = use_bottleneck\n",
    "        self.depth = depth\n",
    "        self.kernel_size = kernel_size - 1\n",
    "        self.callbacks = None\n",
    "        self.batch_size = batch_size\n",
    "        self.bottleneck_size = 32\n",
    "        self.nb_epochs = nb_epochs\n",
    "\n",
    "        if build == True:\n",
    "            self.model = self.build_model(input_shape, nb_classes)\n",
    "            if (verbose == True):\n",
    "                self.model.summary()\n",
    "            self.verbose = verbose\n",
    "            self.model.save_weights(self.output_directory + 'model_init.hdf5')\n",
    "\n",
    "    def _inception_module(self, input_tensor, stride=1, activation='linear'):\n",
    "\n",
    "        if self.use_bottleneck and int(input_tensor.shape[-1]) > 1:\n",
    "            input_inception = keras.layers.Conv1D(filters=self.bottleneck_size, kernel_size=1,\n",
    "                                                  padding='same', activation=activation, use_bias=False)(input_tensor)\n",
    "        else:\n",
    "            input_inception = input_tensor\n",
    "\n",
    "        # kernel_size_s = [3, 5, 8, 11, 17]\n",
    "        kernel_size_s = [self.kernel_size // (2 ** i) for i in range(3)]\n",
    "\n",
    "        conv_list = []\n",
    "\n",
    "        for i in range(len(kernel_size_s)):\n",
    "            conv_list.append(keras.layers.Conv1D(filters=self.nb_filters, kernel_size=kernel_size_s[i],\n",
    "                                                 strides=stride, padding='same', activation=activation, use_bias=False)(\n",
    "                input_inception))\n",
    "\n",
    "        max_pool_1 = keras.layers.MaxPool1D(pool_size=3, strides=stride, padding='same')(input_tensor)\n",
    "\n",
    "        conv_6 = keras.layers.Conv1D(filters=self.nb_filters, kernel_size=1,\n",
    "                                     padding='same', activation=activation, use_bias=False)(max_pool_1)\n",
    "\n",
    "        conv_list.append(conv_6)\n",
    "\n",
    "        x = keras.layers.Concatenate(axis=2)(conv_list)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Activation(activation='relu')(x)\n",
    "        return x\n",
    "\n",
    "    def _shortcut_layer(self, input_tensor, out_tensor):\n",
    "        shortcut_y = keras.layers.Conv1D(filters=int(out_tensor.shape[-1]), kernel_size=1,\n",
    "                                         padding='same', use_bias=False)(input_tensor)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "        x = keras.layers.Add()([shortcut_y, out_tensor])\n",
    "        x = keras.layers.Activation('relu')(x)\n",
    "        return x\n",
    "\n",
    "    def build_model(self, input_shape, nb_classes):\n",
    "        input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "        x = input_layer\n",
    "        input_res = input_layer\n",
    "\n",
    "        for d in range(self.depth):\n",
    "\n",
    "            x = self._inception_module(x)\n",
    "\n",
    "            if self.use_residual and d % 3 == 2:\n",
    "                x = self._shortcut_layer(input_res, x)\n",
    "                input_res = x\n",
    "\n",
    "        gap_layer = keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "        output_layer = keras.layers.Dense(nb_classes, activation='softmax')(gap_layer)\n",
    "\n",
    "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "        lr = 00.5\n",
    "        optimizer = adam_v2.Adam(learning_rate=lr, decay=lr/self.nb_epochs)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50,\n",
    "                                                      min_lr=0.0001)\n",
    "\n",
    "        file_path = self.output_directory + 'best_model.hdf5'\n",
    "\n",
    "        model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss',\n",
    "                                                           save_best_only=True)\n",
    "\n",
    "        self.callbacks = [reduce_lr, model_checkpoint]\n",
    "\n",
    "        return model\n",
    "\n",
    "    def fit(self, x_train, y_train, x_val, y_val, y_true, plot_test_acc=False):\n",
    "        #if len(keras.backend.tensorflow_backend._get_available_gpus()) == 0:       \n",
    "        #    print('error no gpu')\n",
    "        #    exit()\n",
    "        # x_val and y_val are only used to monitor the test loss and NOT for training\n",
    "\n",
    "        if self.batch_size is None:\n",
    "            mini_batch_size = int(min(x_train.shape[0] / 10, 16))\n",
    "        else:\n",
    "            mini_batch_size = self.batch_size\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        if plot_test_acc:\n",
    "\n",
    "            hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=self.nb_epochs,\n",
    "                                  verbose=self.verbose, validation_data=(x_val, y_val), callbacks=self.callbacks)\n",
    "        else:\n",
    "\n",
    "            hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=self.nb_epochs,\n",
    "                                  verbose=self.verbose, callbacks=self.callbacks)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        self.model.save(self.output_directory + 'last_model.hdf5')\n",
    "\n",
    "        y_pred = self.predict(x_val, y_true, x_train, y_train, y_val,\n",
    "                              return_df_metrics=False)\n",
    "\n",
    "        # save predictions\n",
    "        np.save(self.output_directory + 'y_pred.npy', y_pred)\n",
    "\n",
    "        # convert the predicted from binary to integer\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        df_metrics = save_logs(self.output_directory, hist, y_pred, y_true, duration,\n",
    "                               plot_test_acc=plot_test_acc)\n",
    "\n",
    "        keras.backend.clear_session()\n",
    "\n",
    "        return df_metrics\n",
    "\n",
    "    def predict(self, x_test, y_true, x_train, y_train, y_test, return_df_metrics=True):\n",
    "        start_time = time.time()\n",
    "        model_path = self.output_directory + 'best_model.hdf5'\n",
    "        model = keras.models.load_model(model_path)\n",
    "        y_pred = model.predict(x_test, batch_size=self.batch_size)\n",
    "        if return_df_metrics:\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "            df_metrics = calculate_metrics(y_true, y_pred, 0.0)\n",
    "            return df_metrics\n",
    "        else:\n",
    "            test_duration = time.time() - start_time\n",
    "            save_test_duration(self.output_directory + 'test_duration.csv', test_duration)\n",
    "            return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "80oEFjNvtR-C"
   },
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    x_train = datasets_dict[dataset_name][0]\n",
    "    y_train = datasets_dict[dataset_name][1]\n",
    "    x_test = datasets_dict[dataset_name][2]\n",
    "    y_test = datasets_dict[dataset_name][3]\n",
    "\n",
    "    nb_classes = len(np.unique(np.concatenate((y_train, y_test), axis=0)))\n",
    "\n",
    "    # make the min to zero of labels\n",
    "    y_train, y_test = transform_labels(y_train, y_test)\n",
    "\n",
    "    # save orignal y because later we will use binary\n",
    "    y_true = y_test.astype(np.int64)\n",
    "    y_true_train = y_train.astype(np.int64)\n",
    "    # transform the labels from integers to one hot vectors\n",
    "    enc = sklearn.preprocessing.OneHotEncoder()\n",
    "    enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n",
    "    y_train = enc.transform(y_train.reshape(-1, 1)).toarray()\n",
    "    y_test = enc.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "    if len(x_train.shape) == 2:  # if univariate\n",
    "        # add a dimension to make it multivariate with one dimension\n",
    "        x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "        x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "AyhMePQgyywk"
   },
   "outputs": [],
   "source": [
    "def fit_classifier():\n",
    "    input_shape = x_train.shape[1:]\n",
    "    print(\"Fiting classifier..\")\n",
    "    print(f\"classifier_name: {classifier_name}\")\n",
    "    print(f\"input_shape: {input_shape}\")\n",
    "    print(f\"nb_classes: {nb_classes}\")\n",
    "    print(f\"output_directory: {output_directory}\")\n",
    "    \n",
    "    classifier = create_classifier(classifier_name, input_shape, nb_classes,\n",
    "                                   output_directory)\n",
    "\n",
    "    classifier.fit(x_train, y_train, x_test, y_test, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "h_qBGd3dzDjg"
   },
   "outputs": [],
   "source": [
    "def create_classifier(classifier_name, input_shape, nb_classes, output_directory,\n",
    "                      verbose=False, build=True):\n",
    "    if classifier_name == 'nne':\n",
    "        #from classifiers import nne\n",
    "        return Classifier_NNE(output_directory, input_shape, nb_classes, verbose)\n",
    "\n",
    "    if classifier_name == 'inception':\n",
    "        #from classifiers import inception\n",
    "        return Classifier_INCEPTION(output_directory, input_shape, nb_classes,\n",
    "                                    verbose, build=build)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Nbk-HPHpzDw3"
   },
   "outputs": [],
   "source": [
    "def get_xp_val(xp):\n",
    "    if xp == 'batch_size':\n",
    "        xp_arr = [16, 32, 128]\n",
    "    elif xp == 'use_bottleneck':\n",
    "        xp_arr = [False]\n",
    "    elif xp == 'use_residual':\n",
    "        xp_arr = [False]\n",
    "    elif xp == 'nb_filters':\n",
    "        xp_arr = [16, 64]\n",
    "    elif xp == 'depth':\n",
    "        xp_arr = [3, 9]\n",
    "    elif xp == 'kernel_size':\n",
    "        xp_arr = [8, 64]\n",
    "    else:\n",
    "        raise Exception('wrong argument')\n",
    "    return xp_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "rKy73uyc_smT"
   },
   "outputs": [],
   "source": [
    "############################################### main\n",
    "\n",
    "#root_dir = '/b/home/uha/hfawaz-datas/temp-dl-tsc/'\n",
    "\n",
    "root_dir = \"C:\\\\Users\\\\mlb\\\\Teste_VOCs\\\\3_ModelsIMplementations\\\\\"\n",
    "\n",
    "xps = ['use_bottleneck', 'use_residual', 'nb_filters', 'depth',\n",
    "       'kernel_size', 'batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "c78wadTW_uJ6",
    "outputId": "e0ca3a78-a25e-4d73-dafa-9dbc014d23f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mlb\\Teste_VOCs\\3_ModelsIMplementations\\\\archives\\TSC\\AllCandidas\\AllCandidas\n",
      "C:\\Users\\mlb\\Teste_VOCs\\3_ModelsIMplementations\\\\archives\\TSC\\AllCandidas\\AllCandidas_TRAIN.csv\n",
      "C:\\Users\\mlb\\Teste_VOCs\\3_ModelsIMplementations\\\\archives\\TSC\\AllCandidas\\AllCandidas_TEST.csv\n",
      "\t\titer 0\n",
      "\t\t\tdataset_name:  AllCandidas\n",
      "Already_done C:\\Users\\mlb\\Teste_VOCs\\3_ModelsIMplementations\\results\\inception\\TSC\\ AllCandidas\n",
      "\t\titer 1\n",
      "\t\t\tdataset_name:  AllCandidas\n",
      "Already_done C:\\Users\\mlb\\Teste_VOCs\\3_ModelsIMplementations\\results\\inception\\TSC_itr_1\\ AllCandidas\n",
      "\t\titer 2\n",
      "\t\t\tdataset_name:  AllCandidas\n",
      "Already_done C:\\Users\\mlb\\Teste_VOCs\\3_ModelsIMplementations\\results\\inception\\TSC_itr_2\\ AllCandidas\n",
      "\t\titer 3\n",
      "\t\t\tdataset_name:  AllCandidas\n",
      "Already_done C:\\Users\\mlb\\Teste_VOCs\\3_ModelsIMplementations\\results\\inception\\TSC_itr_3\\ AllCandidas\n",
      "\t\titer 4\n",
      "\t\t\tdataset_name:  AllCandidas\n",
      "Already_done C:\\Users\\mlb\\Teste_VOCs\\3_ModelsIMplementations\\results\\inception\\TSC_itr_4\\ AllCandidas\n",
      "\t\titer 5\n",
      "\t\t\tdataset_name:  AllCandidas\n",
      "Already_done C:\\Users\\mlb\\Teste_VOCs\\3_ModelsIMplementations\\results\\inception\\TSC_itr_5\\ AllCandidas\n",
      "\t\titer 6\n",
      "\t\t\tdataset_name:  AllCandidas\n",
      "Already_done C:\\Users\\mlb\\Teste_VOCs\\3_ModelsIMplementations\\results\\inception\\TSC_itr_6\\ AllCandidas\n",
      "\t\titer 7\n",
      "\t\t\tdataset_name:  AllCandidas\n",
      "Already_done C:\\Users\\mlb\\Teste_VOCs\\3_ModelsIMplementations\\results\\inception\\TSC_itr_7\\ AllCandidas\n",
      "\t\titer 8\n",
      "\t\t\tdataset_name:  AllCandidas\n",
      "Already_done C:\\Users\\mlb\\Teste_VOCs\\3_ModelsIMplementations\\results\\inception\\TSC_itr_8\\ AllCandidas\n",
      "\t\titer 9\n",
      "\t\t\tdataset_name:  AllCandidas\n",
      "Already_done C:\\Users\\mlb\\Teste_VOCs\\3_ModelsIMplementations\\results\\inception\\TSC_itr_9\\ AllCandidas\n",
      "C:\\Users\\mlb\\Teste_VOCs\\3_ModelsIMplementations\\\\archives\\TSC\\AllCandidas\\AllCandidas\n",
      "C:\\Users\\mlb\\Teste_VOCs\\3_ModelsIMplementations\\\\archives\\TSC\\AllCandidas\\AllCandidas_TRAIN.csv\n",
      "C:\\Users\\mlb\\Teste_VOCs\\3_ModelsIMplementations\\\\archives\\TSC\\AllCandidas\\AllCandidas_TEST.csv\n",
      "\t\t\tdataset_name:  AllCandidas\n",
      "Fiting classifier..\n",
      "classifier_name: inception\n",
      "input_shape: (11, 1)\n",
      "nb_classes: 6\n",
      "output_directory: C:\\Users\\mlb\\Teste_VOCs\\3_ModelsIMplementations\\results\\inception\\TSC\\AllCandidas\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Pessoais\\AnacondaFiles\\envs\\teste_with_gpu\\lib\\site-packages\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n",
      "C:\\Users\\mlb\\AppData\\Local\\Temp/ipykernel_3132/4161292546.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  res = pd.DataFrame(data=np.zeros((1, 1), dtype=np.float), index=[0],\n",
      "C:\\Users\\mlb\\AppData\\Local\\Temp/ipykernel_3132/3778290522.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  res = pd.DataFrame(data=np.zeros((1, 5), dtype=np.float), index=[0],\n",
      "C:\\Users\\mlb\\AppData\\Local\\Temp/ipykernel_3132/296300641.py:12: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  df_best_model = pd.DataFrame(data=np.zeros((1, 6), dtype=np.float), index=[0],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tDONE\n"
     ]
    }
   ],
   "source": [
    "# run nb_iter_ iterations of Inception on the whole TSC archive\n",
    "classifier_name = 'inception'\n",
    "#archive_name = ARCHIVE_NAMES[0]\n",
    "archive_name = UNIVARIATE_ARCHIVE_NAMES[0]\n",
    "nb_iter_ = 10\n",
    "\n",
    "datasets_dict = read_all_datasets(root_dir, archive_name)\n",
    "\n",
    "for iter in range(nb_iter_):\n",
    "    print('\\t\\titer', iter)\n",
    "\n",
    "    trr = ''\n",
    "    if iter != 0:\n",
    "        trr = '_itr_' + str(iter)\n",
    "\n",
    "    tmp_output_directory = root_dir + 'results\\\\' + classifier_name + '\\\\' + archive_name + trr + '\\\\'\n",
    "\n",
    "    for dataset_name in dataset_names_for_archive[archive_name]:\n",
    "        print('\\t\\t\\tdataset_name: ', dataset_name)\n",
    "\n",
    "        x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = prepare_data()\n",
    "\n",
    "        output_directory = tmp_output_directory + dataset_name + '\\\\'\n",
    "\n",
    "        temp_output_directory = create_directory(output_directory)\n",
    "\n",
    "        if temp_output_directory is None:\n",
    "            print('Already_done', tmp_output_directory, dataset_name)\n",
    "            continue\n",
    "\n",
    "        fit_classifier()\n",
    "\n",
    "        print('\\t\\t\\t\\tDONE')\n",
    "\n",
    "        # the creation of this directory means\n",
    "        create_directory(output_directory + '\\\\DONE')\n",
    "\n",
    "# run the ensembling of these iterations of Inception\n",
    "classifier_name = 'inception'\n",
    "\n",
    "datasets_dict = read_all_datasets(root_dir, archive_name)\n",
    "\n",
    "tmp_output_directory = root_dir + 'results\\\\' + classifier_name + '\\\\' + archive_name + '\\\\'\n",
    "\n",
    "for dataset_name in dataset_names_for_archive[archive_name]:\n",
    "    print('\\t\\t\\tdataset_name: ', dataset_name)\n",
    "\n",
    "    x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = prepare_data()\n",
    "\n",
    "    output_directory = tmp_output_directory + dataset_name + '\\\\'\n",
    "\n",
    "    fit_classifier()\n",
    "\n",
    "    print('\\t\\t\\t\\tDONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "nPraqhai_uWi"
   },
   "outputs": [],
   "source": [
    "## this part is for running inception with the different hyperparameters\n",
    "## listed in the paper, on the whole TSC archive\n",
    "#\n",
    "#archive_name = 'TSC'\n",
    "#classifier_name = 'inception'\n",
    "#max_iterations = 5\n",
    "#\n",
    "#datasets_dict = read_all_datasets(root_dir, archive_name)\n",
    "#\n",
    "#for xp in xps:\n",
    "#\n",
    "#    xp_arr = get_xp_val(xp)\n",
    "#\n",
    "#    print('xp', xp)\n",
    "#\n",
    "#    for xp_val in xp_arr:\n",
    "#        print('\\txp_val', xp_val)\n",
    "#\n",
    "#        kwargs = {xp: xp_val}\n",
    "#\n",
    "#        for iter in range(max_iterations):\n",
    "#\n",
    "#            trr = ''\n",
    "#            if iter != 0:\n",
    "#                trr = '_itr_' + str(iter)\n",
    "#            print('\\t\\titer', iter)\n",
    "#\n",
    "#            for dataset_name in dataset_names_for_archive[archive_name]:\n",
    "#\n",
    "#                output_directory = root_dir + '/results/' + classifier_name + '/' + '/' + xp + '/' + '/' + str(\n",
    "#                    xp_val) + '/' + archive_name + trr + '/' + dataset_name + '/'\n",
    "#\n",
    "#                print('\\t\\t\\tdataset_name', dataset_name)\n",
    "#                x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = prepare_data()\n",
    "#\n",
    "#                # check if data is too big for this gpu\n",
    "#                size_data = x_train.shape[0] * x_train.shape[1]\n",
    "#\n",
    "#                temp_output_directory = create_directory(output_directory)\n",
    "#\n",
    "#                if temp_output_directory is None:\n",
    "#                    print('\\t\\t\\t\\t', 'Already_done')\n",
    "#                    continue\n",
    "#\n",
    "#                input_shape = x_train.shape[1:]\n",
    "#\n",
    "#                #from classifiers import inception\n",
    "#\n",
    "#                classifier = Classifier_INCEPTION(output_directory, input_shape, nb_classes,\n",
    "#                                                  verbose=False, build=True, **kwargs)\n",
    "#\n",
    "#                classifier.fit(x_train, y_train, x_test, y_test, y_true)\n",
    "#\n",
    "#                # the creation of this directory means\n",
    "#                create_directory(output_directory + '/DONE')\n",
    "#\n",
    "#                print('\\t\\t\\t\\t', 'DONE')\n",
    "#\n",
    "## we now need to ensemble each iteration of inception (aka InceptionTime)\n",
    "##archive_name = ARCHIVE_NAMES[0]\n",
    "#archive_name = UNIVARIATE_ARCHIVE_NAMES[0]\n",
    "#classifier_name = 'nne'\n",
    "#\n",
    "#datasets_dict = read_all_datasets(root_dir, archive_name)\n",
    "#\n",
    "#tmp_output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + '/'\n",
    "#\n",
    "#for xp in xps:\n",
    "#    xp_arr = get_xp_val(xp)\n",
    "#    for xp_val in xp_arr:\n",
    "#\n",
    "#        clf_name = 'inception/' + xp + '/' + str(xp_val)\n",
    "#\n",
    "#        for dataset_name in dataset_names_for_archive[archive_name]:\n",
    "#            x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = prepare_data()\n",
    "#\n",
    "#            output_directory = tmp_output_directory + dataset_name + '/'\n",
    "#\n",
    "#            #from classifiers import nne\n",
    "#\n",
    "#            classifier = Classifier_NNE(output_directory, x_train.shape[1:],\n",
    "#                                            nb_classes, clf_name=clf_name)\n",
    "#\n",
    "#            classifier.fit(x_train, y_train, x_test, y_test, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "PV6eyimuzEMs"
   },
   "outputs": [],
   "source": [
    "## this is to generate the archive for the length experiments\n",
    "#run_length_xps(root_dir)\n",
    "#\n",
    "#clfs = []\n",
    "#itr = '-0-1-2-3-4-'\n",
    "#inceptionTime = 'nne/inception'\n",
    "## add InceptionTime: an ensemble of 5 Inception networks\n",
    "#clfs.append(inceptionTime + itr)\n",
    "## add InceptionTime for each hyperparameter study\n",
    "#for xp in xps:\n",
    "#    xp_arr = get_xp_val(xp)\n",
    "#    for xp_val in xp_arr:\n",
    "#        clfs.append(inceptionTime + '/' + xp + '/' + str(xp_val) + itr)\n",
    "#df = generate_results_csv('results.csv', root_dir, clfs)\n",
    "#print(df)\n",
    "#\n",
    "#\n",
    "#df_clfs = pd.DataFrame(clfs)\n",
    "#arq = 'df_clfs.csv'\n",
    "#filepath = root_dir + '/' + arq\n",
    "#df_clfs.to_csv(filepath)\n",
    "#\n",
    "#arq = 'df.csv'\n",
    "#filepath = root_dir + '/' + arq\n",
    "#df.to_csv(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GVjkoASEn1Uh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ORgKD2Y8n1W2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  classifier_name archive_name dataset_name  ord1  ord2  iteration  precision  \\\n",
      "0             0.0          0.0          0.0   0.0   0.0        0.0        0.0   \n",
      "1             0.0          0.0          0.0   0.0   0.0        0.0        0.0   \n",
      "2             0.0          0.0          0.0   0.0   0.0        0.0        0.0   \n",
      "3             0.0          0.0          0.0   0.0   0.0        0.0        0.0   \n",
      "4             0.0          0.0          0.0   0.0   0.0        0.0        0.0   \n",
      "5             0.0          0.0          0.0   0.0   0.0        0.0        0.0   \n",
      "6             0.0          0.0          0.0   0.0   0.0        0.0        0.0   \n",
      "7             0.0          0.0          0.0   0.0   0.0        0.0        0.0   \n",
      "8             0.0          0.0          0.0   0.0   0.0        0.0        0.0   \n",
      "9             0.0          0.0          0.0   0.0   0.0        0.0        0.0   \n",
      "\n",
      "   accuracy  recall  duration  \n",
      "0       0.0     0.0       0.0  \n",
      "1       0.0     0.0       0.0  \n",
      "2       0.0     0.0       0.0  \n",
      "3       0.0     0.0       0.0  \n",
      "4       0.0     0.0       0.0  \n",
      "5       0.0     0.0       0.0  \n",
      "6       0.0     0.0       0.0  \n",
      "7       0.0     0.0       0.0  \n",
      "8       0.0     0.0       0.0  \n",
      "9       0.0     0.0       0.0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def gen_results_csv(clf_name, path, root_dir):\n",
    "\n",
    "    ARCHIVE_NAMES = ['TSC', 'TSC_itr_1', 'TSC_itr_2', 'TSC_itr_3', 'TSC_itr_4', 'TSC_itr_5',\n",
    "                     'TSC_itr_6', 'TSC_itr_7', 'TSC_itr_8', 'TSC_itr_9'] \n",
    "    \n",
    "    DATASET_NAME = ['anem_col_1006e20072021'] \n",
    "   \n",
    "    res1 = np.zeros((10, 3))\n",
    "    \n",
    "    res1 = pd.DataFrame(res1)\n",
    "    \n",
    "    res1 = res1.astype(object)\n",
    "    \n",
    "    nomes = ['classifier_name', 'archive_name', 'dataset_name']\n",
    "    res1.columns=nomes\n",
    "    \n",
    "    res2 = np.zeros((10, 7))\n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "    for archive_name in ARCHIVE_NAMES:\n",
    "    \n",
    "        o = 0\n",
    "        for dataset_name in DATASET_NAME:\n",
    "    \n",
    "            output_dir = root_dir + path + archive_name + '\\\\' \\\n",
    "                         + dataset_name + '\\\\' + 'df_metrics.csv'\n",
    "            #print(output_dir)\n",
    "            #print('j = ',j,'i = ',i)\n",
    "            if not os.path.exists(output_dir):\n",
    "                continue\n",
    "            df_metrics = pd.read_csv(output_dir)\n",
    "            res1['classifier_name'][j] = clf_name\n",
    "            res1['archive_name'][j] = archive_name\n",
    "            res1['dataset_name'][j] = dataset_name\n",
    "            res2[j,0] = j\n",
    "            res2[j,1] = o\n",
    "            res2[j,2] = i\n",
    "            res2[j,3] = df_metrics['precision']\n",
    "            res2[j,4] = df_metrics['accuracy']\n",
    "            res2[j,5] = df_metrics['recall']\n",
    "            res2[j,6] = df_metrics['duration']\n",
    "            \n",
    "            o += 1\n",
    "            j += 1\n",
    "        i += 1\n",
    "    \n",
    "    \n",
    "    res2 = pd.DataFrame(res2)\n",
    "    nomes = ['ord1', 'ord2', 'iteration', 'precision', 'accuracy', 'recall', 'duration']\n",
    "    res2.columns=nomes\n",
    "    \n",
    "    \n",
    "    res = pd.concat([res1, res2], ignore_index=True, axis=1)\n",
    "    nomes = ['classifier_name', 'archive_name', 'dataset_name','ord1', 'ord2',\n",
    "             'iteration', 'precision', 'accuracy', 'recall', 'duration']\n",
    "    res.columns=nomes\n",
    "    \n",
    "    res = res.sort_values(by=['ord2', 'ord1'])\n",
    "    \n",
    "    return res\n",
    "\n",
    "root_dir = \"C:\\\\Users\\\\mlb\\\\Teste_VOCs\\\\3_ModelsIMplementations\\\\results\"\n",
    "\n",
    "clf_name = 'candidas_inception_result'\n",
    "\n",
    "path = '\\\\'\n",
    "\n",
    "res = gen_results_csv(clf_name, path, root_dir)\n",
    "\n",
    "print(res)\n",
    "\n",
    "arq = clf_name + '.csv'\n",
    "filepath = root_dir + path + arq\n",
    "res.to_csv(filepath, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Inceptiomtime_implementation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
